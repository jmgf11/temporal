# üÜì Gu√≠a: Usar Ollama (100% GRATIS)

## ¬øQu√© es Ollama?

Ollama es una herramienta que te permite ejecutar **modelos de IA localmente en tu computadora, completamente gratis**. Sin API keys, sin costos, sin l√≠mites.

## üéØ Ventajas vs Claude/OpenAI

| Caracter√≠stica | Ollama (Gratis) | Claude/OpenAI (Pago) |
|----------------|-----------------|----------------------|
| **Precio** | üü¢ $0 - Gratis siempre | üî¥ $3-75 por mill√≥n tokens |
| **Privacidad** | üü¢ 100% local, tus datos no salen | üü° Se env√≠an a servidores externos |
| **L√≠mites** | üü¢ Sin l√≠mites, usa cuanto quieras | üî¥ L√≠mites por API key |
| **Velocidad** | üü° Depende de tu hardware | üü¢ Muy r√°pido (servidores potentes) |
| **Calidad** | üü° Buena, pero menos que Claude | üü¢ Excelente |
| **Requisitos** | üî¥ 8-16GB RAM | üü¢ Solo internet |

## üìã Requisitos para Ollama

### Hardware M√≠nimo:
- **RAM**: 8GB (recomendado 16GB)
- **Disco**: 10-20GB libres
- **CPU**: Cualquier procesador moderno
- **GPU**: Opcional (NVIDIA mejora velocidad 10x)

### Modelos Disponibles y sus Requisitos:

| Modelo | RAM Necesaria | Calidad | Velocidad | Recomendado para |
|--------|---------------|---------|-----------|------------------|
| **llama3.2:1b** | 2GB | ‚≠ê‚≠ê | üöÄüöÄüöÄ | Pruebas r√°pidas |
| **llama3.2:3b** | 4GB | ‚≠ê‚≠ê‚≠ê | üöÄüöÄ | Uso general b√°sico |
| **llama3.1:8b** | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | üöÄ | **Recomendado** |
| **llama3.1:70b** | 48GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üêå | Servers potentes |
| **mistral:7b** | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | üöÄ | Alternativa r√°pida |
| **codellama:7b** | 8GB | ‚≠ê‚≠ê‚≠ê‚≠ê | üöÄ | Programaci√≥n |
| **phi3:3.8b** | 4GB | ‚≠ê‚≠ê‚≠ê | üöÄüöÄ | Eficiente |

## üöÄ Instalaci√≥n de Ollama en Ubuntu

### Paso 1: Instalar Ollama

```bash
# Instalaci√≥n con un comando
curl -fsSL https://ollama.ai/install.sh | sh

# Verificar instalaci√≥n
ollama --version
```

### Paso 2: Descargar un Modelo

```bash
# Modelo recomendado (8GB RAM)
ollama pull llama3.1

# O si tienes poca RAM (4GB)
ollama pull llama3.2:3b

# O para programaci√≥n
ollama pull codellama
```

Esto descargar√° el modelo (puede tardar 5-15 minutos la primera vez).

### Paso 3: Probar Ollama

```bash
# Iniciar chat interactivo
ollama run llama3.1

# Probar con un mensaje
>>> Hola, ¬øc√≥mo est√°s?
>>> ¬øQu√© puedes hacer?

# Salir: /bye
```

### Paso 4: Iniciar Servidor Ollama

```bash
# El servidor debe estar corriendo para que el agente lo use
ollama serve

# O en background
nohup ollama serve > /dev/null 2>&1 &
```

## ‚öôÔ∏è Configurar el Agente para Usar Ollama

### Opci√≥n 1: Solo Ollama (100% Gratis)

Edita tu `.env`:

```env
# Dejar vac√≠as las API keys de pago
ANTHROPIC_API_KEY=
OPENAI_API_KEY=

# Configurar Ollama
OLLAMA_MODEL=llama3.1
OLLAMA_BASE_URL=http://localhost:11434
```

### Opci√≥n 2: Ollama como Fallback (H√≠brido)

```env
# Claude mientras tengas cr√©dito gratis
ANTHROPIC_API_KEY=sk-ant-api03-tu_clave_aqui

# Ollama como backup cuando se acabe el cr√©dito
OLLAMA_MODEL=llama3.1
OLLAMA_BASE_URL=http://localhost:11434
```

El agente **autom√°ticamente cambiar√° a Ollama** cuando:
- Se acaben los cr√©ditos de Claude
- Haya un error con la API de Claude
- Claude no est√© disponible

## üéÆ Usar el Agente con Ollama

```bash
# 1. Asegurarte que Ollama est√© corriendo
ollama serve &

# 2. Activar entorno virtual
source venv/bin/activate

# 3. Iniciar agente
python main.py cli

# Ver√°s algo como:
# ‚úÖ Proveedor activo: Ollama (llama3.1) - GRATIS
```

## üìä Comparaci√≥n de Modelos

He probado varios, aqu√≠ est√° mi recomendaci√≥n:

### Para tu Agente Aut√≥nomo:

**1. llama3.1:8b** ‚≠ê RECOMENDADO
```bash
ollama pull llama3.1
```
- Buen balance calidad/velocidad
- Entiende bien instrucciones complejas
- Funciona bien con herramientas

**2. mistral:7b** ‚≠ê ALTERNATIVA
```bash
ollama pull mistral
```
- M√°s r√°pido que llama
- Bueno para tareas espec√≠ficas
- Menos "conversacional"

**3. codellama:7b** ‚≠ê PARA PROGRAMACI√ìN
```bash
ollama pull codellama
```
- Especializado en c√≥digo
- Excelente para debugging
- No tan bueno para chat general

### Si tienes Poca RAM (4GB):

**phi3:3.8b**
```bash
ollama pull phi3:3.8b
```
- Modelo peque√±o pero capaz
- Funciona en 4GB RAM
- Sorprendentemente bueno

## üîß Configuraci√≥n Avanzada de Ollama

### Cambiar Puerto

```bash
# En .env
OLLAMA_BASE_URL=http://localhost:11435
```

```bash
# Iniciar Ollama en puerto diferente
OLLAMA_HOST=0.0.0.0:11435 ollama serve
```

### Usar GPU NVIDIA (Acelera 10x)

Si tienes una GPU NVIDIA:

```bash
# Instalar CUDA toolkit
sudo apt install nvidia-cuda-toolkit

# Ollama autom√°ticamente usar√° la GPU
ollama run llama3.1

# Verificar uso de GPU
nvidia-smi
```

### Optimizar para CPU

Edita `.env`:
```env
# N√∫mero de threads (ajusta seg√∫n tus cores)
OLLAMA_NUM_THREADS=4

# Context window m√°s peque√±o (usa menos RAM)
OLLAMA_NUM_CTX=2048
```

## üí° Tips y Trucos

### 1. Modelo para Cada Tarea

Puedes cambiar de modelo seg√∫n la tarea:

```python
# En .env, usa el modelo general
OLLAMA_MODEL=llama3.1

# Pero puedes crear perfiles:
# Para programaci√≥n: codellama
# Para chat: llama3.1
# Para an√°lisis: mistral
```

### 2. Precalentar el Modelo

```bash
# Mant√©n el modelo "caliente" para respuestas m√°s r√°pidas
ollama run llama3.1 <<< "Hola"
```

### 3. Limpiar Modelos Viejos

```bash
# Ver modelos instalados
ollama list

# Eliminar modelo
ollama rm nombre-modelo

# Esto libera espacio en disco
```

### 4. Actualizar Ollama

```bash
# Actualizar a la √∫ltima versi√≥n
curl -fsSL https://ollama.ai/install.sh | sh

# Re-descargar modelos mejorados
ollama pull llama3.1
```

## üêõ Soluci√≥n de Problemas

### "connection refused" o "failed to connect"

```bash
# El servidor no est√° corriendo, in√≠cialo:
ollama serve &

# Verificar que est√° corriendo:
curl http://localhost:11434/api/tags
```

### "out of memory" o modelo muy lento

```bash
# Usa un modelo m√°s peque√±o
ollama pull llama3.2:3b

# O limita el context window en .env:
OLLAMA_NUM_CTX=2048
```

### Modelo da respuestas extra√±as

```bash
# Re-descargar el modelo
ollama rm llama3.1
ollama pull llama3.1
```

## üìà Mejorando el Rendimiento

### 1. Aumentar RAM de la VM

Si tu agente es lento:
- Aumenta la RAM de tu VM a 16GB
- Cierra otras aplicaciones
- Usa modelo m√°s peque√±o

### 2. Usar SSD

- Ollama funciona MUY lento en HDD
- Aseg√∫rate que la VM est√© en SSD

### 3. Optimizar Context Window

En `.env`:
```env
# Reducir para usar menos RAM
OLLAMA_NUM_CTX=4096  # Default es 8192
```

## üéØ Estrategia Recomendada

### Plan "Mejor de Ambos Mundos":

**Fase 1: Aprender (Gratis)**
1. Usa los $5 gratis de Claude
2. Experimenta, aprende c√≥mo funciona
3. Cuando se acaben, pasa a Ollama

**Fase 2: Producci√≥n (H√≠brido)**
1. Ollama para tareas rutinarias (gratis)
2. Claude para tareas cr√≠ticas (pagas lo que uses)
3. El agente cambia autom√°ticamente

**Fase 3: Escalar**
1. Si necesitas m√°s potencia: paga Claude
2. Si quieres ahorrar: solo Ollama
3. ¬°T√∫ decides!

## üÜö Comparaci√≥n Pr√°ctica

He probado ambos con el mismo prompt:

**Prompt:** "Lista los archivos en /home y crea un reporte"

| Aspecto | Claude Sonnet | Ollama llama3.1 |
|---------|---------------|-----------------|
| Tiempo | 2.3s | 8.5s |
| Precisi√≥n | 10/10 | 8/10 |
| Costo | $0.02 | $0.00 |
| Calidad respuesta | Excelente | Buena |

**Veredicto:** Ollama es perfectamente usable para el 80% de tareas.

## üìö Recursos Adicionales

- [Ollama Official](https://ollama.ai/)
- [Modelos Disponibles](https://ollama.ai/library)
- [Ollama en GitHub](https://github.com/ollama/ollama)
- [Comparaci√≥n de Modelos](https://github.com/ollama/ollama#model-library)

---

## ‚úÖ Checklist Final

Antes de empezar con Ollama:

- [ ] VM con m√≠nimo 8GB RAM
- [ ] 15GB de espacio libre en disco
- [ ] Ollama instalado: `ollama --version`
- [ ] Modelo descargado: `ollama list`
- [ ] Servidor corriendo: `ollama serve &`
- [ ] `.env` configurado con `OLLAMA_MODEL=llama3.1`
- [ ] Probado: `ollama run llama3.1`

---

**¬°Listo para usar IA de forma 100% gratuita!** üéâ

¬øDudas? Revisa el README o los logs del agente.
